{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7edf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# yarn 클러스터매니저를 사용하기 위해 yarn conf path 지정\n",
    "os.environ[\"YARN_CONF_DIR\"] = \"/mcw/spark3/conf2\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"yarn-spark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\") \\\n",
    "    .config(\"spark.executor.instances\", \"5\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752f627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "# 모델 로드\n",
    "rf_model = RandomForestClassificationModel.load(\"hdfs://spark-master-01:9000/mcw/model/ml_model_pf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7cbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka 데이터 소스 설정\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"spark-worker-01:9092,spark-worker-02:9092,spark-worker-03:9092\") \\\n",
    "    .option(\"subscribe\", \"heat\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786d1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#스키마 설정\n",
    "schema = StructType([\n",
    "    StructField(\"R상무효전력\", DoubleType(), True),\n",
    "    StructField(\"R상선간전압\", DoubleType(), True),\n",
    "    StructField(\"R상유효전력\", DoubleType(), True),\n",
    "    StructField(\"R상전류\", DoubleType(), True),\n",
    "    StructField(\"R상전압\", DoubleType(), True),\n",
    "    StructField(\"S상무효전력\", DoubleType(), True),\n",
    "    StructField(\"S상선간전압\", DoubleType(), True),\n",
    "    StructField(\"S상유효전력\", DoubleType(), True),\n",
    "    StructField(\"S상전류\", DoubleType(), True),\n",
    "    StructField(\"S상전압\", DoubleType(), True),\n",
    "    StructField(\"T상무효전력\", DoubleType(), True),\n",
    "    StructField(\"T상선간전압\", DoubleType(), True),\n",
    "    StructField(\"T상유효전력\", DoubleType(), True),\n",
    "    StructField(\"T상전류\", DoubleType(), True),\n",
    "    StructField(\"T상전압\", DoubleType(), True),\n",
    "    StructField(\"누적전력량\", DoubleType(), True),\n",
    "    StructField(\"무효전력평균\", DoubleType(), True),\n",
    "    StructField(\"상전압평균\", DoubleType(), True),\n",
    "    StructField(\"선간전압평균\", DoubleType(), True),\n",
    "    StructField(\"온도\", DoubleType(), True),\n",
    "    StructField(\"유효전력평균\", DoubleType(), True),\n",
    "    StructField(\"전류평균\", DoubleType(), True),\n",
    "    StructField(\"주파수\", DoubleType(), True),\n",
    "    StructField(\"sensor_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c6a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 07:22:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# JSON 데이터 파싱\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# 데이터 전처리\n",
    "preprocessed_df = parsed_df.select(\n",
    "    struct(\n",
    "        col(\"sensor_id\"),\n",
    "        col(\"온도\"),\n",
    "        when(col(\"유효전력평균\") != 0, expr(\"`유효전력평균` / sqrt(pow(`유효전력평균`, 2) + pow(`무효전력평균`, 2))\")).otherwise(0).alias(\"pf\"),\n",
    "        when(col(\"R상유효전력\") != 0, expr(\"`R상유효전력` / sqrt(pow(`R상유효전력`, 2) + pow(`R상무효전력`, 2))\")).otherwise(0).alias(\"R_pf\"),\n",
    "        when(col(\"S상유효전력\") != 0, expr(\"`S상유효전력` / sqrt(pow(`S상유효전력`, 2) + pow(`S상무효전력`, 2))\")).otherwise(0).alias(\"S_pf\"),\n",
    "        when(col(\"T상유효전력\") != 0, expr(\"`T상유효전력` / sqrt(pow(`T상유효전력`, 2) + pow(`T상무효전력`, 2))\")).otherwise(0).alias(\"T_pf\"),\n",
    "    ).alias(\"data\")\n",
    ")\n",
    "\n",
    "\n",
    "# 특성 벡터 생성\n",
    "feature_cols = [\"온도\", \"pf\", \"R_pf\", \"S_pf\", \"T_pf\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "input_df = assembler.transform(preprocessed_df.select(\"data.*\"))\n",
    "\n",
    "# 예측 수행\n",
    "predictions = rf_model.transform(input_df)\n",
    "\n",
    "# 예측값을 포함하여 데이터 전처리\n",
    "preprocessed_df2 = predictions.select(\n",
    "            struct(\n",
    "                col(\"sensor_id\"),\n",
    "                col(\"온도\"),\n",
    "                col(\"pf\"),\n",
    "                col(\"R_pf\"),\n",
    "                col(\"S_pf\"),\n",
    "                col(\"T_pf\"),\n",
    "                when(col(\"prediction\") == 2.0, \"경고\").otherwise(when(col(\"prediction\") == 1.0, \"주의\").otherwise(\"정상\")).alias(\"pred\")\n",
    "            ).alias(\"data\")\n",
    "        )\n",
    "\n",
    "\n",
    "# JSON 문자열로 변환\n",
    "output_df = preprocessed_df2.selectExpr(\"to_json(data) AS value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959b179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 09:41:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# 배치 df 확인\n",
    "def check_df(batch_df, batch_id):\n",
    "    batch_df.show()\n",
    "    \n",
    "# spark warehouse에 저장하는 쿼리\n",
    "writeDW = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://spark-master-01:9000/checkpoint/writeDW\") \\\n",
    "    .option(\"path\", \"hdfs://spark-master-01:9000/mcw/raw\") \\\n",
    "    .queryName(\"writeDW\") \\\n",
    "    .start()\n",
    "\n",
    "# # 스트림 실행\n",
    "# spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6395f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDW.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b35c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 09:41:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 데이터 처리를 한 후 다시 kafka에 보내는 쿼리\n",
    "pf_pred_to_kafka = output_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"spark-worker-01:9092,spark-worker-02:9092,spark-worker-03:9092\") \\\n",
    "    .option(\"topic\", \"heat_pf\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://spark-master-01:9000/checkpoint/writestreaming\") \\\n",
    "    .queryName(\"pf_pred_to_kafka\") \\\n",
    "    .start()\n",
    "\n",
    "# 스트리밍 실행\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36d90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_pred_to_kafka.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6aafd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92cbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
