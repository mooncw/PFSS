{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7edf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-02f547c0-36f9-4312-afe8-b5553c559b2d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1125ms :: artifacts dl 56ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-02f547c0-36f9-4312-afe8-b5553c559b2d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/26ms)\n",
      "23/05/24 07:19:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/24 07:19:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar added multiple times to distributed cache.\n",
      "23/05/24 07:19:12 WARN Client: Same path resource file:///home/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# yarn 클러스터매니저를 사용하기 위해 yarn conf path 지정\n",
    "os.environ[\"YARN_CONF_DIR\"] = \"/mcw/spark3/conf2\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"yarn-spark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\") \\\n",
    "    .config(\"spark.executor.instances\", \"5\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752f627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "# 모델 로드\n",
    "rf_model = RandomForestClassificationModel.load(\"hdfs://spark-master-01:9000/mcw/model/ml_model_pf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7cbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka 데이터 소스 설정\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"spark-worker-01:9092,spark-worker-02:9092,spark-worker-03:9092\") \\\n",
    "    .option(\"subscribe\", \"heat\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786d1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#스키마 설정\n",
    "schema = StructType([\n",
    "    StructField(\"R상무효전력\", DoubleType(), True),\n",
    "    StructField(\"R상선간전압\", DoubleType(), True),\n",
    "    StructField(\"R상역률\", DoubleType(), True),\n",
    "    StructField(\"R상유효전력\", DoubleType(), True),\n",
    "    StructField(\"R상전류\", DoubleType(), True),\n",
    "    StructField(\"R상전류고조파\", DoubleType(), True),\n",
    "    StructField(\"R상전압\", DoubleType(), True),\n",
    "    StructField(\"R상전압고조파\", DoubleType(), True),\n",
    "    StructField(\"S상무효전력\", DoubleType(), True),\n",
    "    StructField(\"S상선간전압\", DoubleType(), True),\n",
    "    StructField(\"S상역률\", DoubleType(), True),\n",
    "    StructField(\"S상유효전력\", DoubleType(), True),\n",
    "    StructField(\"S상전류\", DoubleType(), True),\n",
    "    StructField(\"S상전류고조파\", DoubleType(), True),\n",
    "    StructField(\"S상전압\", DoubleType(), True),\n",
    "    StructField(\"S상전압고조파\", DoubleType(), True),\n",
    "    StructField(\"T상무효전력\", DoubleType(), True),\n",
    "    StructField(\"T상선간전압\", DoubleType(), True),\n",
    "    StructField(\"T상역률\", DoubleType(), True),\n",
    "    StructField(\"T상유효전력\", DoubleType(), True),\n",
    "    StructField(\"T상전류\", DoubleType(), True),\n",
    "    StructField(\"T상전류고조파\", DoubleType(), True),\n",
    "    StructField(\"T상전압\", DoubleType(), True),\n",
    "    StructField(\"T상전압고조파\", DoubleType(), True),\n",
    "    StructField(\"누적전력량\", DoubleType(), True),\n",
    "    StructField(\"무효전력평균\", DoubleType(), True),\n",
    "    StructField(\"상전압평균\", DoubleType(), True),\n",
    "    StructField(\"선간전압평균\", DoubleType(), True),\n",
    "    StructField(\"역률평균\", DoubleType(), True),\n",
    "    StructField(\"온도\", DoubleType(), True),\n",
    "    StructField(\"유효전력평균\", DoubleType(), True),\n",
    "    StructField(\"전류고조파평균\", DoubleType(), True),\n",
    "    StructField(\"전류평균\", DoubleType(), True),\n",
    "    StructField(\"전압고조파평균\", DoubleType(), True),\n",
    "    StructField(\"주파수\", DoubleType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"sensor_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c6a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 07:22:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# JSON 데이터 파싱\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# 데이터 전처리\n",
    "preprocessed_df = parsed_df.select(\n",
    "    struct(\n",
    "        col(\"sensor_id\"),\n",
    "        col(\"온도\"),\n",
    "        when(col(\"유효전력평균\") != 0, expr(\"`유효전력평균` / sqrt(pow(`유효전력평균`, 2) + pow(`무효전력평균`, 2))\")).otherwise(0).alias(\"pf\"),\n",
    "        when(col(\"R상유효전력\") != 0, expr(\"`R상유효전력` / sqrt(pow(`R상유효전력`, 2) + pow(`R상무효전력`, 2))\")).otherwise(0).alias(\"R_pf\"),\n",
    "        when(col(\"S상유효전력\") != 0, expr(\"`S상유효전력` / sqrt(pow(`S상유효전력`, 2) + pow(`S상무효전력`, 2))\")).otherwise(0).alias(\"S_pf\"),\n",
    "        when(col(\"T상유효전력\") != 0, expr(\"`T상유효전력` / sqrt(pow(`T상유효전력`, 2) + pow(`T상무효전력`, 2))\")).otherwise(0).alias(\"T_pf\"),\n",
    "    ).alias(\"data\")\n",
    ")\n",
    "\n",
    "\n",
    "# 특성 벡터 생성\n",
    "feature_cols = [\"온도\", \"pf\", \"R_pf\", \"S_pf\", \"T_pf\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "input_df = assembler.transform(preprocessed_df.select(\"data.*\"))\n",
    "\n",
    "# 예측 수행\n",
    "predictions = rf_model.transform(input_df)\n",
    "\n",
    "# 예측값을 포함하여 데이터 전처리\n",
    "preprocessed_df2 = predictions.select(\n",
    "            struct(\n",
    "                col(\"sensor_id\"),\n",
    "                col(\"온도\"),\n",
    "                col(\"pf\"),\n",
    "                col(\"R_pf\"),\n",
    "                col(\"S_pf\"),\n",
    "                col(\"T_pf\"),\n",
    "                when(col(\"prediction\") == 2.0, \"경고\").otherwise(when(col(\"prediction\") == 1.0, \"주의\").otherwise(\"정상\")).alias(\"pred\")\n",
    "            ).alias(\"data\")\n",
    "        )\n",
    "\n",
    "\n",
    "# JSON 문자열로 변환\n",
    "output_df = preprocessed_df2.selectExpr(\"to_json(data) AS value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959b179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 09:41:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# 배치 df 확인\n",
    "def check_df(batch_df, batch_id):\n",
    "    batch_df.show()\n",
    "    \n",
    "# spark warehouse에 저장하는 쿼리\n",
    "writeDW = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://spark-master-01:9000/checkpoint/writeDW\") \\\n",
    "    .option(\"path\", \"hdfs://spark-master-01:9000/mcw/raw\") \\\n",
    "    .queryName(\"writeDW\") \\\n",
    "    .start()\n",
    "\n",
    "# # 스트림 실행\n",
    "# spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6395f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDW.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b35c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 09:41:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 데이터 처리를 한 후 다시 kafka에 보내는 쿼리\n",
    "pf_pred_to_kafka = output_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"spark-worker-01:9092,spark-worker-02:9092,spark-worker-03:9092\") \\\n",
    "    .option(\"topic\", \"heat_pf\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://spark-master-01:9000/checkpoint/writestreaming\") \\\n",
    "    .queryName(\"pf_pred_to_kafka\") \\\n",
    "    .start()\n",
    "\n",
    "# 스트리밍 실행\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36d90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_pred_to_kafka.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6aafd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92cbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
